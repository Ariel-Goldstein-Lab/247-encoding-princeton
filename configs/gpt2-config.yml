
# Embeddings
emb: gpt2-xl
context_length: 1024
layer_idx: 48

# Embedding modifications: none, rand, emb, shift-emb, concat-emb
emb_mod: shift-emb

pca_to: 0
ridge: False
himalaya: True

# Lags
lags: np.arange(-10000,10001,100)
output_dir_name: nopca-ols-lag100k-100-all-himalaya
