import csv
import os
from functools import partial
from imp import C_EXTENSION
from multiprocessing import Pool

import mat73
import numpy as np
import pandas as pd
from numba import jit, prange
from scipy import stats
from scipy.spatial.distance import pdist, squareform
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GroupKFold, KFold
from sklearn.pipeline import make_pipeline


def encColCorr(CA, CB):
    """[summary]

    Args:
        CA ([type]): [description]
        CB ([type]): [description]

    Returns:
        [type]: [description]
    """
    df = np.shape(CA)[0] - 2

    CA -= np.mean(CA, axis=0)
    CB -= np.mean(CB, axis=0)

    r = np.sum(CA * CB, 0) / np.sqrt(np.sum(CA * CA, 0) * np.sum(CB * CB, 0))

    t = r / np.sqrt((1 - np.square(r)) / df)
    p = stats.t.sf(t, df)

    r = r.squeeze()

    if r.size > 1:
        r = r.tolist()
    else:
        r = float(r)

    return r, p, t


def cv_lm_003_prod_comp(args, Xtra, Ytra, fold_tra, Xtes, Ytes, fold_tes, lag):
    if lag == -1:
        print("running regression")
    else:
        print("running regression with best_lag")

    if args.pca_to == 0 or "nopca" in args.datum_mod:
        print(f"No PCA, emb_dim = {Xtes.shape[1]}")
    else:
        print(f"PCA from {Xtes.shape[1]} to {args.pca_to}")

    nSamps = Xtes.shape[0]
    nChans = Ytra.shape[1] if Ytra.shape[1:] else 1

    YHAT = np.zeros((nSamps, nChans))
    Ynew = np.zeros((nSamps, nChans))

    for i in range(0, args.fold_num):
        Xtraf, Xtesf = Xtra[fold_tra != i], Xtes[fold_tes == i]
        Ytraf, Ytesf = Ytra[fold_tra != i], Ytes[fold_tes == i]

        # Xtesf -= np.mean(Xtraf, axis=0)
        # Xtraf -= np.mean(Xtraf, axis=0)
        Ytesf -= np.mean(Ytraf, axis=0)
        Ytraf -= np.mean(Ytraf, axis=0)

        # Fit model
        if args.pca_to == 0 or "nopca" in args.datum_mod:
            model = make_pipeline(StandardScaler(), LinearRegression())
        else:
            model = make_pipeline(
                StandardScaler(),
                PCA(args.pca_to, whiten=True),
                LinearRegression(),
            )
        model.fit(Xtraf, Ytraf)

        if lag != -1:
            B = model.named_steps["linearregression"].coef_
            assert lag < B.shape[0], f"Lag index out of range"
            B = np.repeat(B[lag, :][np.newaxis, :], B.shape[0], 0)  # best-lag model
            model.named_steps["linearregression"].coef_ = B

        # Predict
        foldYhat = model.predict(Xtesf)

        Ynew[fold_tes == i, :] = Ytesf.reshape(-1, nChans)
        YHAT[fold_tes == i, :] = foldYhat.reshape(-1, nChans)

    return (YHAT, Ynew)


def cv_lm_003_prod_comp_new(args, Xtra, Ytra, fold_tra, Xtes, Ytes, fold_tes, lag):
    """
    Used for whisper revision 1. Predicting best lag based on train and predict on
    test with set lag and get one correlation per fold
    """

    nSamps = Xtes.shape[0]
    YHAT = np.zeros((nSamps, 1))
    Ynew = np.zeros((nSamps, 1))

    rps = []
    for i in range(0, args.fold_num):
        Xtraf, Xtesf = Xtra[fold_tra != i], Xtes[fold_tes == i]
        Ytraf, Ytesf = Ytra[fold_tra != i], Ytes[fold_tes == i]

        Ytesf -= np.mean(Ytraf, axis=0)
        Ytraf -= np.mean(Ytraf, axis=0)

        # Fit model
        if args.pca_to == 0 or "nopca" in args.datum_mod:
            model = make_pipeline(StandardScaler(), LinearRegression())
        else:
            model = make_pipeline(
                StandardScaler(),
                PCA(args.pca_to, whiten=True),
                LinearRegression(),
            )
        model.fit(Xtraf, Ytraf)

        # pick best lag from train
        foldYhat = model.predict(Xtraf)
        rp, _, _ = encColCorr(Ytraf, foldYhat)
        lag = np.argmax(np.array(rp))

        # predict for test
        foldYhat = model.predict(Xtesf)
        rp, _, _ = encColCorr(Ytesf, foldYhat)
        rps.append([rp[lag]])

        Ynew[fold_tes == i, :] = Ytesf[:, lag].reshape(-1, 1)
        YHAT[fold_tes == i, :] = foldYhat[:, lag].reshape(-1, 1)

    # Add whole datum correlation
    rp, _, _ = encColCorr(Ynew, YHAT)
    rps = [[rp]] + rps
    return rps


@jit(nopython=True)
def build_Y(onsets, convo_onsets, convo_offsets, brain_signal, lags, window_size):
    """[summary]

    Args:
        onsets ([type]): [description]
        brain_signal ([type]): [description]
        lags ([type]): [description]
        window_size ([type]): [description]

    Returns:
        [type]: [description]
    """
    half_window = round((window_size / 1000) * 512 / 2)

    # Y1 = np.zeros((len(onsets), len(lags), 2 * half_window + 1))
    Y1 = np.zeros((len(onsets), len(lags)))

    for lag in prange(len(lags)):
        lag_amount = int(lags[lag] / 1000 * 512)

        index_onsets = np.minimum(
            convo_offsets - half_window - 1,
            np.maximum(
                convo_onsets + half_window + 1,
                np.round_(onsets, 0, onsets) + lag_amount,
            ),
        )

        # index_onsets = np.round_(onsets, 0, onsets) + lag_amount

        # subtracting 1 from starts to account for 0-indexing
        starts = index_onsets - half_window - 1
        stops = index_onsets + half_window

        # vec = brain_signal[np.array(
        #     [np.arange(*item) for item in zip(starts, stops)])]

        for i, (start, stop) in enumerate(zip(starts, stops)):
            Y1[i, lag] = np.mean(brain_signal[start:stop].reshape(-1))

    return Y1


def build_XY(args, datum, brain_signal):
    """[summary]

    Args:
        args ([type]): [description]
        datum ([type]): [description]
        brain_signal ([type]): [description]

    Returns:
        [type]: [description]
    """
    X = np.stack(datum.embeddings).astype("float64")

    word_onsets = datum.adjusted_onset.values
    convo_onsets = datum.convo_onset.values
    convo_offsets = datum.convo_offset.values

    lags = np.array(args.lags)
    brain_signal = brain_signal.reshape(-1, 1)
    Y = build_Y(
        word_onsets,
        convo_onsets,
        convo_offsets,
        brain_signal,
        lags,
        args.window_size,
    )

    return X, Y


def encoding_mp_prod_comp(
    args, Xtra, Ytra, fold_tra, Xtes, Ytes, fold_tes, lag, fold_cor=True
):
    if args.shuffle:
        np.random.shuffle(Ytra)
        np.random.shuffle(Ytes)

    PY_hat, Y_new = cv_lm_003_prod_comp(
        args, Xtra, Ytra, fold_tra, Xtes, Ytes, fold_tes, lag
    )

    rps = []
    # correlation for whole datum
    rp, _, _ = encColCorr(Y_new, PY_hat)
    rps.append(rp)

    # correlation per folds
    for i in np.unique(fold_tes).astype(int):
        rp_fold, _, _ = encColCorr(Y_new[fold_tes == i], PY_hat[fold_tes == i])
        rps.append(rp_fold)

    if fold_cor:  # correlation per fold
        return rps
    else:
        return rp


def noise_ceiling(args, elec_name, datum, signal, mode, rep=10):
    # specify prod or comp
    if mode == "comp":
        datum = datum[datum.speaker != "Speaker1"].copy()
    elif mode == "prod":
        datum = datum[datum.speaker == "Speaker1"].copy()

    # sample datum
    datum.loc[:, "embeddings"] = datum["word"].groupby(datum["word"]).transform("count")
    datum = datum[datum.embeddings >= rep]
    datum = datum.groupby("word").sample(n=rep, random_state=42)

    # get signal
    _, Y = build_XY(args, datum, signal)
    assert Y.shape[0] % rep == 0
    word_num = int(Y.shape[0] / rep)
    Y = Y.reshape(word_num, rep, Y.shape[1])

    # compute corr matrix
    rps = []
    for i in np.arange(0, Y.shape[2]):
        rps.append(1 - pdist(Y[:, :, i], "correlation"))
    rps = np.vstack(rps)

    # aggregate summaries
    rps_result = []
    rps_result.append(rps.min(axis=1))
    rps_result.append(np.percentile(rps, 25, axis=1))
    rps_result.append(np.percentile(rps, 50, axis=1))
    rps_result.append(np.percentile(rps, 75, axis=1))
    rps_result.append(rps.max(axis=1))
    rps_result.append(rps.mean(axis=1))
    rps_result.append(np.std(rps, axis=1))
    rps_result.append(stats.sem(rps, axis=1))
    rps_result = np.vstack(rps_result)

    # write file
    trial_str = append_jobid_to_string(args, mode)
    filename = os.path.join(args.full_output_dir, elec_name + trial_str + ".csv")
    with open(filename, "w") as csvfile:
        print("writing file")
        csvwriter = csv.writer(csvfile)
        csvwriter.writerows(rps_result)
    # filename = os.path.join(args.full_output_dir, elec_name + trial_str + ".npy")
    # np.save(filename, rps)
    return word_num


def run_regression(args, Xtra, Ytra, fold_tra, Xtes, Ytes, fold_tes):
    perm_prod = []
    for i in range(args.npermutations):
        if args.model_mod and "best-lag" in args.model_mod:
            result = encoding_mp_prod_comp(
                args, Xtra, Ytra, fold_tra, Xtes, Ytes, fold_tes, -1, False
            )
            best_lag = np.argmax(np.array(result))
            print("switch to best-lag: " + str(best_lag))
            perm_prod.append(
                encoding_mp_prod_comp(
                    args, Xtra, Ytra, fold_tra, Xtes, Ytes, fold_tes, best_lag
                )
            )
        elif args.model_mod and "pred-lag" in args.model_mod:
            perm_prod.append(
                cv_lm_003_prod_comp_new(
                    args, Xtra, Ytra, fold_tra, Xtes, Ytes, fold_tes, -1
                )
            )
        else:
            result = encoding_mp_prod_comp(
                args, Xtra, Ytra, fold_tra, Xtes, Ytes, fold_tes, -1
            )
            perm_prod.append(result)

    return perm_prod


def get_groupkfolds(datum, X, Y, fold_num=10):
    fold_cat = np.zeros(datum.shape[0])
    grpkfold = GroupKFold(n_splits=fold_num)
    folds = [t[1] for t in grpkfold.split(X, Y, groups=datum["conversation_id"])]

    for i in range(0, len(folds)):
        for row in folds[i]:
            fold_cat[row] = i  # turns into fold category

    fold_cat_prod = fold_cat[datum.speaker == "Speaker1"]
    fold_cat_comp = fold_cat[datum.speaker != "Speaker1"]

    return (fold_cat_prod, fold_cat_comp)


def get_kfolds(X, fold_num=10):
    print("Using kfolds")
    skf = KFold(n_splits=fold_num, shuffle=False)
    folds = [t[1] for t in skf.split(np.arange(X.shape[0]))]
    fold_cat = np.zeros(X.shape[0])
    for i in range(0, len(folds)):
        for row in folds[i]:
            fold_cat[row] = i  # turns into fold category
    return fold_cat


def write_encoding_results(args, cor_results, elec_name, mode):
    """Write output into csv files

    Args:
        args (namespace): commandline arguments
        cor_results: correlation results
        elec_name: electrode name as a substring of filename
        mode: 'prod' or 'comp'

    Returns:
        None
    """
    trial_str = append_jobid_to_string(args, mode)
    filename = os.path.join(args.full_output_dir, elec_name + trial_str + ".csv")
    fold_filename = os.path.join(
        args.full_output_dir, elec_name + trial_str + "_fold.csv"
    )

    if len(cor_results) == 1:  # no permutations
        cor_results = cor_results[0]

    # correlation for whole datum
    cor_datum = [cor_results[0]]
    with open(filename, "w") as csvfile:
        print("writing file")
        csvwriter = csv.writer(csvfile)
        csvwriter.writerows(cor_datum)

    # correlation per fold
    cor_folds = cor_results[1:]
    df = pd.DataFrame(cor_folds)
    df.loc[len(df), :] = df.mean(axis=0)
    df.loc[len(df), :] = df.sem(axis=0, ddof=0)
    # FIXME I really don't like mean and sem here, maybe we do it in plotting instead?
    df.to_csv(fold_filename, index=False)

    return None


def append_jobid_to_string(args, speech_str):
    """Adds job id to the output eletrode.csv file.

    Args:
        args (Namespace): Contains all commandline agruments
        speech_str (string): Production (prod)/Comprehension (comp)

    Returns:
        string: concatenated string
    """
    speech_str = "_" + speech_str

    if args.job_id:
        trial_str = "_".join([speech_str, f"{args.job_id:02d}"])
    else:
        trial_str = speech_str

    return trial_str
